---
title: Concept of PCA
description: 紀錄Machine learning PCA的筆記
date: 2025-05-31
tages: ["note"]
---

# Introduce PCA

- PCA use the Feature Extraction
- PCA is the unsupervised linear transformation technique
- PCA 最突出的特點是對由dataset consisting of many correlated features進行降維，同時保留資料集的變化(variation)
## The Difference between Feature Selecion and Feature Extraction

- Feature Selcetion: the selected features are maintanined(feature 並不會有改變) like the original one
- Feature Extraction: use feature extraction to **transform** or project the data onto a `new feature space` (feature和之前的不一樣)
  - Feature Extraction not only imporve storage space(efficiency) but also **reduces** the overfiting due to the curse of dimensionality(維度災難) in non-regularized models
### orthogonal axes
$$
\begin{bmatrix}
x \\
y \\
z \\
\end{bmatrix}
=
x \cdot
\begin{bmatrix}
1 \\
0 \\
0 \\
\end{bmatrix}
+
y \cdot
\begin{bmatrix}
0 \\
1 \\
0 \\
\end{bmatrix}
+
z \cdot
\begin{bmatrix}
0 \\
0 \\
1 \\
\end{bmatrix}
$$

## Concept of PCA
- 在PCA裡, $X$ , $Y$ , $Z$ 軸被稱爲principle componements(主要元素)
- PCA uses an orthogonal transformation(正交變換) 將features投影到new spcae
- 在滿足new space下的axes皆互為正交的前提下,orthogonal axes可以解釋(interpreted)為最大變異數的方向
  - 正交的目的是為了讓featurs不具備關聯性

- PCA降維的前提必須要構建一個轉移矩陣
$$
z = xW \in \mathbb{R}^{1 \times k}, \text{typically}\ k \ll d
$$
  - 第一主成份數據中將會獲得最大變異量,他捕捉了數據中主要變化的方向
  - 接下來的主成份$N_i$在和$N_\text{i-1}$為**正交**的前提下依次補捉剩餘數據中的最大變異量
  - 主成份的數量最多可以和原先保持一致,但在降維的角度下要選則較少的原始特徵
- 因為PCA為高度sensitive的data scaling,因此需要standardize the features **prior** to PCA

## Summarizing PCA in a Few Steps
1. 標準化 (Standardize) 原始的 $d$ 維數據集
2. 建構標準化後數據集的共變異數矩陣(covariance matrix)
3. 將共變異數矩陣分解為其特徵向量 (eigenvectors) 和特徵值 (eigenvalues)
4. 選擇與 $k$ 個最大特徵值相對應的 $k$ 個特徵向量，其中 $k$ 是新特徵子空間的維度 $k < d$
5. 建構一個投影矩陣 (projection matrix) $W$，其列向量 (columns) 是選出的 $k$ 個特徵向量
6. 使用投影矩陣 $W$ 轉換 $d$ 維的輸入數據集，以獲得新的 $k$ 維特徵子空間

# Superised Data Compression via Linear Discriminant Analysis

## PCA vs LDA
